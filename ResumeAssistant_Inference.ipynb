{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjoL7Nmx2be9T7nrdDl0jg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axiom19/Resume-Keyword-Assistant/blob/main/ResumeAssistant_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Tailor Machine\n",
        "\n",
        "Motivation: Applying to job makes you use Jobscan, which tells us the missing keywords. Sometimes its hard to incorporate keywords without changing the sense of the statement.\n",
        "So the following project assists in adding the keyword to your resume.\n",
        "\n",
        "This notebook implements a Resume Tailor Machine using Gradio and the Hugging Face Gemma 2B model. The application allows users to upload a resume and input a keyword. The AI then incorporates the keyword into the resume in a natural way, improving its relevance for specific job applications."
      ],
      "metadata": {
        "id": "9pT7UHjDygYw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G8iHmmqir8lo"
      },
      "outputs": [],
      "source": [
        "# ! pip install gradio transformers langchain langchain-huggingface pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U langchain-community"
      ],
      "metadata": {
        "id": "ULffmJ8514z2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "4415Gf6g0ZR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import statements\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import gradio as gr\n",
        "import torch\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "9kFtWmHlyVTH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# declare global variables\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "g0O7keAR1zBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76895ba4-94d2-455c-d96e-8f04ef448b4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Initialization\n",
        "\n",
        "Here we initialize the Hugging Face model with specific parameters. We're using the google/gemma-2b-it model for this application."
      ],
      "metadata": {
        "id": "yFVPGOUY0W1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize LLM\n",
        "\n",
        "model_params = {\n",
        "    'temperature': 0.2,\n",
        "    'max_length': 512,\n",
        "    'repetition_penalty': 1.2,\n",
        "    'top_p': 0.9\n",
        "}\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    model_kwargs=model_params,\n",
        "    huggingfacehub_api_token=os.environ['HF_TOKEN']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A87acOGrYzA-",
        "outputId": "b78554f1-799d-4a2b-ef4d-5289a3b5cae9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering\n",
        "\n",
        "This section defines the prompt template used to guide the AI in modifying the resume. The prompt instructs the AI to incorporate the given keyword naturally into the resume content."
      ],
      "metadata": {
        "id": "d_NAS4kn0fwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model template\n",
        "\n",
        "template = \"\"\"\n",
        "You are an AI assistant specialized in improving resumes. Given the following resume content and a single keyword, add this keyword naturally into the resume. If the keyword is already present, emphasize it or use it more effectively.\n",
        "\n",
        "Resume Content:\n",
        "{resume_content}\n",
        "\n",
        "Keyword to incorporate:\n",
        "{keyword}\n",
        "\n",
        "Add or modify the keyword in the resume and return the following:\n",
        "1. The section name where the change was made\n",
        "2. The full line where the keyword was added or modified\n",
        "3. A one liner explanation of why this change improves the resume\n",
        "\n",
        "Format your response as follows:\n",
        "Section: [section name]\n",
        "Modified Line: [full modified line]\n",
        "Explanation: [one liner explanation]\n",
        "\n",
        "If the keyword cannot be appropriately added to the resume, explain why.\n",
        "\"\"\"\n",
        "\n",
        "# create prompt template\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"resume_content\", \"keyword\"]\n",
        ")\n",
        "\n",
        "# create langchain\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS0e9E2vhOM2",
        "outputId": "155f920b-6440-46ab-edf8-fd235345679d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing\n"
      ],
      "metadata": {
        "id": "CGI-nJSK0lAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract data from resume\n",
        "\n",
        "def extract_data(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    document = loader.load()\n",
        "\n",
        "    return document[0].page_content"
      ],
      "metadata": {
        "id": "MZCRvBP-h1jf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resume processing function\n",
        "\n",
        "def process_resume(file_path, keyword):\n",
        "    # load the resume\n",
        "    resume = extract_data(file_path)\n",
        "\n",
        "    # process the resume\n",
        "    response = llm_chain.run(resume_content=resume, keyword=keyword)\n",
        "\n",
        "\n",
        "    section = re.search(r'Section: (.+)', response)\n",
        "    modified_line = re.search(r'Modified Line: (.+)', response)\n",
        "    explanation = re.search(r'Explanation: (.+)', response)\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"Keyword: {keyword}\\n\\n\"\n",
        "    if section and modified_line and explanation:\n",
        "        output += f\"Section: {section.group(1)}\\n\"\n",
        "        output += f\"Modified Line: {modified_line.group(1)}\\n\"\n",
        "        output += f\"Explanation: {explanation.group(1)}\\n\"\n",
        "    else:\n",
        "        output += \"The keyword could not be appropriately added to the resume.\\n\"\n",
        "        output += f\"Explanation: {response}\\n\"\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "rcdRISg3mHGI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process_resume(file_path=\"/content/resume.pdf\", keyword=\"innovative\")"
      ],
      "metadata": {
        "id": "vTtpz80ip4pi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interface Creation"
      ],
      "metadata": {
        "id": "HA6PVGcQ0lz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create UI with Gradio\n",
        "\n",
        "inputs = [\n",
        "    gr.File(label=\"Upload Resume\"),\n",
        "    gr.Textbox(label=\"Keyword\")\n",
        "]\n",
        "\n",
        "outputs = gr.Textbox(label=\"Output\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    fn=process_resume,\n",
        "    title=\"Resume Tailor Machine\",\n",
        "    description=\"Upload a resume and a keyword to add the keyword and improve the match-score\",\n",
        ")"
      ],
      "metadata": {
        "id": "386pUovOqCto"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "9-k0oyLQxrvc",
        "outputId": "ae611ae2-d3fd-4c3c-d4c0-0ba24ed5e5c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3bc40299170457d023.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3bc40299170457d023.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlWjzTvbx0pc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}